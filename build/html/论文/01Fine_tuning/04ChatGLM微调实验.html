<!DOCTYPE html>
<html class="no-js" lang="zh-CN">
<head>
    <meta charset="utf-8" />
        <meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    
        <title>ChatGLM微调实验 &mdash; SphinxDiary v1.0 文档</title>
    
    <link rel="stylesheet" type="text/css" href="../../_static/dist/fontawesome.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/dist/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
            <link rel="index" title="索引" href="../../genindex.html" />
            <link rel="search" title="搜索" href="../../search.html" />
            <link rel="top" title="SphinxDiary v1.0 文档" href="#" />
            <link rel="up" title="Fine tuning" href="../1-index.html" />
            <link rel="next" title="语音识别" href="../2-index.html" />
            <link rel="prev" title="LoRA" href="03Lora.html" />
    </head>
<body>
    <script type="text/javascript" src="../../_static/dist/blocking.js"></script>
    <header class="container-fluid bg-primary">
        <a class="btn btn-sm btn-light skip-to-content-link" href="#main">Skip to content</a>
        <div class="container-fluid">
            <div class="navbar navbar-expand-lg navbar-dark font-weight-bold">
                    <a href="../../index.html"
                        title="Wagtail"
                        class="logo navbar-brand"
                    >
                        <img src="../../_static/img/wagtail-logo-new.svg" width="45" height="59" alt="Wagtail"
                            class="logo-img"
                        />
                        Sphinx Wagtail Theme
                    </a>
                
                
                <button class="navbar-toggler btn btn-primary d-lg-none" type="button" data-toggle="collapse" data-target="#collapseSidebar" aria-expanded="false" aria-controls="collapseExample">
                    <span class="navbar-toggler-icon"></span>
                    <span class="sr-only">menu</span>
                </button>
            </div>
        </div>
    </header>
    <div class="container-fluid">
        <div class="row">
            <aside class="col-12 col-lg-3 sidebar-container">
                <div id="collapseSidebar" class="collapse sticky-top d-lg-block pt-5 pr-lg-4">
<div id="searchbox" class="searchbox mb-6 px-1" role="search">
    <form id="search-form" action="../../search.html" autocomplete="off" method="get" role="search">
        <div class="input-group">
            <div class="input-group-prepend">
                <div class="input-group-text border-right-0 bg-white py-3 pl-3 pr-2"><span class="fas fa-search"></span></div>
            </div>
            <input class="form-control py-3 pr-3 pl-1 h-100 border-left-0" type="search" name="q" placeholder="Search documentation" aria-label="Search documentation" id="searchinput" />
        </div>
        <input type="hidden" name="check_keywords" value="yes" />
        <input type="hidden" name="area" value="default" />
    </form>
</div><div class="site-toc">
    <nav class="toc mt-3" aria-label="Main menu">
        <p class="caption" role="heading"><span class="caption-text">笔记:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/1-index.html">Docker</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/01Docker/01Docker%E6%96%B0%E5%BB%BA%E7%94%A8%E6%88%B7.html">Docker新建用户</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/01Docker/02Docker%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C.html">Docker基本操作</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/01Docker/03Docker%E5%AE%B9%E5%99%A8%E5%A4%87%E4%BB%BD%E4%B8%8E%E8%BF%81%E7%A7%BB.html">Docker容器备份与迁移</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/2-index.html">Kubernetes</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/02Triton_with_kubernetes/01%E5%9C%A8Ubuntu18.04%E4%B8%8A%E6%90%AD%E5%BB%BAkubernetes.html">kubernetes安装</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/02Triton_with_kubernetes/02%E4%BD%BF%E7%94%A8MIG%E5%92%8CKubernetes%E5%A4%A7%E8%A7%84%E6%A8%A1%E9%83%A8%E7%BD%B2NVIDIA%20Triton.html">Kubernetes部署NVIDIA Triton</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/02Triton_with_kubernetes/03%E4%BD%BF%E7%94%A8Kubernetes%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4.html">Kubernetes创建集群</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/02Triton_with_kubernetes/04Triton%20Metrics.html">Triton Metrics</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/02Triton_with_kubernetes/05Grafana.html">Grafana</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/3-index.html">Triton</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/03Triton/01Triton_Inference_Server%E5%85%A5%E9%97%A8.html">Triton Inference Server入门</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/03Triton/02Triton%E5%85%A5%E9%97%A8%E7%BA%A7%E6%95%99%E7%A8%8B.html">Triton入门级教程</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/03Triton/03Triton_Backend%E8%AF%A6%E8%A7%A3.html">Triton_Backend详解</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/4-index.html">Pytorch</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/04Pytorch/01%E6%95%B0%E6%8D%AE%E9%9B%86.html">数据集</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/5-index.html">语音识别环境部署教程</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/05%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/01jetpack5.0.2%E5%AE%89%E8%A3%85.html">Jetpack 5.0.2安装</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/05%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/02%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE.html">深度学习环境配置</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/05%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/03%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E6%A8%A1%E5%9E%8B%E9%83%A8%E7%BD%B2.html">语音识别模型部署</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/05%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/04%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86.html">客户端模型推理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/05%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/05%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95.html">性能测试</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/6-index.html">小工具</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/01Git.html">Git</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/2-index.html">Sphinx</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/02Sphinx/01GitHub%20%2B%20Spinx%20%2B%20Read%20the%20docs%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%B8%80%EF%BC%89.html">GitHub + Spinx + Read the docs实战入门指南（一）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/02Sphinx/02GitHub%20%2B%20Spinx%20%2B%20Read%20the%20docs%E5%AE%9E%E6%88%98%E5%85%A5%E9%97%A8%E6%8C%87%E5%8D%97%EF%BC%88%E4%BA%8C%EF%BC%89.html">GitHub + Spinx + Read the docs实战入门指南（二）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/02Sphinx/03Markdown.html">Markdown</a></li>
<li class="toctree-l3"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/02Sphinx/04MyST-Parser.html">MyST-Parser</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/03Fastertransformer.html">Fastertransformer</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/04FileBrowser.html">FileBrowser</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/05NFS%E9%85%8D%E7%BD%AE.html">NFS配置</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/06%E6%9C%8D%E5%8A%A1%E5%99%A8%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86%E5%B7%A5%E5%85%B7.html">服务器文件管理工具</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/06%E5%B0%8F%E5%B7%A5%E5%85%B7/07StableDiffusion.html">Stable Diffusion</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/7-index.html">NNI</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/07NNI/01-NNI%E7%A4%BA%E4%BE%8B.html">NNI 示例</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/07NNI/02-NNI%E9%83%A8%E7%BD%B2.html">NNI 部署</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/8-index.html">Android</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/08Android/01Android%20APK.html">Android APK</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/9-index.html">Android Studio</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/09Android%20Studio/01Android%E4%BD%BF%E7%94%A8.html">1.导入.aar文件</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/09Android%20Studio/01Android%E4%BD%BF%E7%94%A8.html#app-build-gradle-dependencies">2.在app/build.gradle dependencies中加入：</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../%E7%AC%94%E8%AE%B0/09Android%20Studio/01Android%E4%BD%BF%E7%94%A8.html#id1">3.重新编译工程</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">论文:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../1-index.html">Fine tuning</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="01Prompt%20Tuning.html">Prompt Tuning</a></li>
<li class="toctree-l2"><a class="reference internal" href="02Prompt%20Tuning%20v2.html">Prompt Tuning V2</a></li>
<li class="toctree-l2"><a class="reference internal" href="03Lora.html">LoRA</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">ChatGLM微调实验</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../2-index.html">语音识别</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/1-index.html">语音识别综述</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/01%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0/1-Recent%20Advance%20in%20End-to-End%20Automatic%20Speech%20Recognition.html">1-Recent Advance in End-to-End Automatic Speech Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/01%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB%E7%BB%BC%E8%BF%B0/2-A%20Comparative%20Study%20On%20Transformer%20VS%20RNN%20In%20Speech%20Applications.html">2-A Comparative Study On Transformer VS RNN In Speech Applications</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/2-index.html">Wenet</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/02Wenet/1-WeNet%20Production%20Oriented%20Streaming%20and%20Non-Streaming%20End-to-End%20Speech%20Recognition%20Toolkit.html">1-WeNet Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/02Wenet/2-WeNet2.0%20More%20Productive%20End-to-End%20Speech%20Recognition%20Toolkit.html">2-Wenet2.0 More Productive End-to-End Speech Recognition Toolkit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/02Wenet/3-WeNet-LM%E6%A8%A1%E5%9E%8B.html">3-WeNet-LM模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/02Wenet/4-WeNet%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84.html">4-Wenet模型结构</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/02Wenet/5-WeNet%E5%AE%9E%E9%AA%8C%E5%B0%8F%E7%BB%93.html">5-WeNet实验小结</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/02Wenet/6-WeNet%E9%83%A8%E7%BD%B2.html">6-WeNet部署</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/3-index.html">模型微调</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/03%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83/1-FINE-TUNING%20OF%20PRE-TRAINED%20END-TO-END%20SPEECH%20RECOGNITION%20WITH%20GENERATIVE%20ADVERSARIAL%20NETWORKS.html">1-Fine-Tuning Of Pre-trained end-to-end Speech Recognition With Generative Adversarial Networks</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/4-index.html">编码器</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/04%E7%BC%96%E7%A0%81%E5%99%A8/1-Conformer-Convolution-augmented%20Transformer%20for%20Speech%20Recognition.html">1-Conformer-Convolution-augmented Transformer for Speech Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/04%E7%BC%96%E7%A0%81%E5%99%A8/2-Paraformer-Fast%20and%20Accurate%20Parallel%20Transformer%20for%20Non-autoregressive.html">2-Paraformer-Fast and Accurate Parallel Transformer for Non-autoregressive</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/04%E7%BC%96%E7%A0%81%E5%99%A8/3-LFEformer-Local_Feature_Enhancement_Using_Sliding_Window_With_Deformability_for_Automatic_Speech_Recognition.html">3-LFEformer-Local Feature Enhancement Using Sliding Window With Deformability for Automatic Speech Recognition</a></li>
<li class="toctree-l3"><a class="reference internal" href="../02%E8%AF%AD%E9%9F%B3%E8%AF%86%E5%88%AB/04%E7%BC%96%E7%A0%81%E5%99%A8/4-Squeezeformer-An%20Efficient%20Transformer%20for%20Automatic%20Speech%20Recognition.html">4-Squeezeformer-An Efficient Transformer for Automatic Speech Recognition</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../3-index.html">联邦学习</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../03%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/1-index.html">联邦学习项目</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../03%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0/01%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E9%A1%B9%E7%9B%AE/1-%E8%81%94%E9%82%A6%E5%AD%A6%E4%B9%A0%E5%BC%80%E6%BA%90%E9%A1%B9%E7%9B%AE.html">联邦学习开源项目</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../4-index.html">语言模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../04%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B/1-Transformer-XL-Attention%20Language%20Models%20Beyond%20a%20Fixed-Length%20Context.html">1-Transformer-XL-Attention Language Models Beyond a Fixed-Length Context</a></li>
</ul>
</li>
</ul>

    </nav>
    <template data-toggle-item-template>
        <button class="btn btn-sm btn-link toctree-expand" type="button">
            <span class="sr-only">Toggle menu contents</span>
        </button>
    </template>
</div>
                    <div class="d-lg-none border-bottom">
                        
                    </div>
                </div>
            </aside>
            <div class="col-12 col-lg-9 pt-5">
                <header class="row align-items-baseline">
                    <div class="col">
                        <nav aria-label="breadcrumb">
    <ol class="breadcrumb m-0 p-0 bg-transparent">
        <li class="breadcrumb-item"><a href="../../index.html">Docs</a></li>
            <li class="breadcrumb-item"><a href="../1-index.html">Fine tuning</a></li>
        <li class="breadcrumb-item active" aria-current="page">ChatGLM微调实验</li>
    </ol>
</nav>
                    </div>
                    <div class="col-sm-12 col-lg-auto mt-3 mt-lg-3">
                        <noscript>
                            <p>JavaScript is required to toggle light/dark mode..</p>
                        </noscript>
                        <button id="wagtail-theme" class="btn btn-sm btn-light text-decoration-none" type="button">
                            <span class="dark-only"><i class="fas fa-sun"></i> Light mode</span>
                            <span class="light-only"><i class="fas fa-moon"></i> Dark mode</span>
                        </button>
    <a class="btn btn-sm btn-light text-decoration-none" href="https://github.com/wagtail/sphinx_wagtail_theme/blob/main/docs/论文/01Fine_tuning/04ChatGLM微调实验.md" rel="nofollow">
        <span class="btn-icon"><span class="fab fa-github"></span></span>
        <span class="btn-text">Edit on GitHub</span>
    </a>
    <a class="btn btn-sm btn-light text-decoration-none" href="../../_sources/论文/01Fine_tuning/04ChatGLM微调实验.md.txt" rel="nofollow">
        <span class="btn-icon"><span class="fas fa-code"></span></span>
        <span class="btn-text">View source</span>
    </a>
                        
                    </div>
                </header>
                <div class="row" >
                    <div class="col-12">
                        <hr class="w-100 my-4">
                    </div>
                </div>
                <div class="row">
                    <div class="col-12 col-lg-9 order-last order-lg-first rst-content">
                        <main role="main" id="main">
    <section class="tex2jax_ignore mathjax_ignore" id="chatglm">
<h1>ChatGLM微调实验<a class="headerlink" href="#chatglm" title="此标题的永久链接">¶</a></h1>
<p>微调实验参考链接：<a class="reference external" href="https://github.com/liucongg/ChatGLM-Finetuning">https://github.com/liucongg/ChatGLM-Finetuning</a></p>
<p><strong>预训练AI大模型</strong></p>
<p>AI预训练大模型网址：<a class="reference external" href="https://www.datalearner.com/ai-models/pretrained-models">最近几年AI模型列表</a></p>
<p><img alt="" src="../../_images/image-20230509163925799.png" /></p>
<p>AI模型月报</p>
<p><img alt="" src="../../_images/image-20230509164042167.png" /></p>
<p>AI大模型预览图：</p>
<p><img alt="" src="../../_images/1.png" /></p>
<p><strong>ChatGLM-6B模型结构</strong></p>
<p>​	ChatGLM-6B的底层架构是通用语言模型（GLM），GLM利用自回归空白填充作为其主要的预训练目标，它掩盖了随机的连续文本区间，并对其进行自回归预测，采用两种不同的掩码标识符[MASK]和[gMASK]分别用于短文和长文的生成</p>
<p><img alt="" src="../../_images/image-20230509170917122.png" /></p>
<p>​	ChatGLM-6B是清华大学知识工程和数据挖掘小组发布的一个开源的对话机器人，约60亿参数的中英文语言模型，并对中文做了优化。</p>
<ul class="simple">
<li><p>28层Transformer，采用<code class="docutils literal notranslate"><span class="pre">final_layernorm</span></code>进行输出</p></li>
<li><p>每一层的结构：<code class="docutils literal notranslate"><span class="pre">input_layernorm</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">attention.query_key_value</span></code> -&gt;<code class="docutils literal notranslate"><span class="pre">attention.dense</span></code>-&gt; <code class="docutils literal notranslate"><span class="pre">post_attention_layernorm</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">mlp.dense_h_to_4h</span></code> -&gt; <code class="docutils literal notranslate"><span class="pre">mlp.dense_4h_to_h</span></code></p></li>
<li><p>总共参数6173286400</p></li>
</ul>
<p><strong>多卡部署</strong></p>
<p>如果你有多张 GPU，但是每张 GPU 的显存大小都不足以容纳完整的模型，那么可以将模型切分在多张GPU上。首先安装 accelerate: <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">accelerate</span></code>，然后通过如下方法加载模型：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">utils</span> <span class="kn">import</span> <span class="n">load_model_on_gpus</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">load_model_on_gpus</span><span class="p">(</span><span class="s2">&quot;../chatglm-6b&quot;</span><span class="p">,</span> <span class="n">num_gpus</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>即可将模型部署到两张 GPU 上进行推理。</p>
<p>单卡部署显存占用：</p>
<p><img alt="" src="../../_images/image-20230509201910217.png" /></p>
<p>多卡部署显存占用：</p>
<p><img alt="" src="../../_images/image-20230509201720223.png" /></p>
<p>在多卡部署模式下，可以进行推理，但无法进行训练，会出现错误：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cuda:0! (when checking argument for argument weight in method wrapper__native_layer_norm)
</pre></div>
</div>
<p><strong>模型部署</strong></p>
<ul>
<li><p>代码调用生成对话</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span><span class="p">,</span> <span class="n">AutoModel</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">os</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;CUDA_VISIBLE_DEVICES&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s1">&#39;1&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;../chatglm-6b&quot;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">AutoModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;../chatglm-6b&quot;</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">half</span><span class="p">()</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;你好&quot;</span><span class="p">,</span> <span class="n">history</span><span class="o">=</span><span class="p">[])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">你好👋!我是人工智能助手 ChatGLM-6B,很高兴见到你,欢迎问我任何问题。</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">response</span><span class="p">,</span> <span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">chat</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="s2">&quot;晚上睡不着应该怎么办&quot;</span><span class="p">,</span> <span class="n">history</span><span class="o">=</span><span class="n">history</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">response</span><span class="p">)</span>
<span class="go">晚上睡不着可能会让人感到困扰和焦虑，但有一些方法可以帮助你入睡。以下是一些可能有用的技巧：</span>

<span class="go">1. 创造一个舒适的睡眠环境：确保卧室安静、黑暗、凉爽和舒适。你可以使用柔和的音乐或白噪声、窗帘或眼罩来帮助创造舒适的睡眠环境。</span>

<span class="go">2. 建立一个睡前例行程序：每天晚上在相同的时间上床，并建立一个睡前例行程序，如洗澡、读书或听轻柔的音乐，有助于让身体和大脑准备好睡觉。</span>

<span class="go">3. 避免使用电子设备：在睡觉前尽量避免使用电子设备，如手机、电脑和电视，因为这些设备会发出蓝光，影响身体释放褪黑素，这是帮助入睡的关键物质。</span>

<span class="go">4. 尝试放松技巧：放松技巧，如深呼吸、渐进性肌肉松弛和冥想，可以帮助你放松身体和头脑，更容易入睡。</span>

<span class="go">5. 避免在床上做其他事情：避免在床上做与睡眠无关的事情，如看电视、使用电脑或手机等，这将有助于确保身体和大脑进入睡眠状态。</span>

<span class="go">如果使用了这些方法仍然无法入睡，你可能需要寻求医生的建议，因为可能存在某些睡眠障碍或其他健康问题，需要进一步的治疗。</span>
</pre></div>
</div>
</li>
<li><p>网页版Demo</p></li>
<li><p>命令行Demo</p>
<p><img alt="" src="../../_images/image-20230510201219981.png" /></p>
</li>
</ul>
<p><strong>三元组抽取任务</strong></p>
<ul class="simple">
<li><p>采用一个领域的比赛数据集-<a class="reference external" href="https://www.datafountain.cn/competitions/584">汽车工业故障模式关系抽取</a>，随机抽取50条作为测试集</p></li>
<li><p>本任务为信息抽取中的关系抽取任务，带抽取的文本语料为工业制造领域相关故障案例文本</p></li>
<li><p>部件单元（燃油泵、换流变压器、分离器）、性能表征（压力、转速、温度）、故障状态（漏油、断裂、变形、卡滞）</p></li>
</ul>
<p>未微调：</p>
<p><img alt="" src="../../_images/image-20230425200038369.png" /></p>
<p>理想答案：</p>
<p><img alt="" src="../../_images/image-20230425200015879.png" /></p>
<p><strong>Freeze方法</strong></p>
<p>​	Freeze方法，即参数冻结，对原始模型部分参数进行冻结操作，仅训练部分参数，以达到在单卡或不进行TP或PP操作，就可以对大模型进行训练。核心代码如下：</p>
<p><img alt="" src="../../_images/image-20230509171421842.png" /></p>
<p>训练采用<a class="reference external" href="https://www.deepspeed.ai/">DeepSpeed</a>进行训练。deep speed是微软的新大规模模型分布式训练的工具，专门为训练超大模型而生，号称可以训练10B参数的模型，比目前最好的模型大10倍，训练速度块10倍，兼容pytorch的模型，改动最少代码。</p>
<ul class="simple">
<li><p>模型训练时，最大长度为768，Batch size为2，训练轮数为5，fp16训练，仅训练模型的后5层参数</p></li>
<li><p>训练示例</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>prompt_text = 你现在是一个信息抽取模型，请你帮我抽取出关系内容为\&quot;性能故障\&quot;, \&quot;部件故障\&quot;, \&quot;组成\&quot;和 \&quot;检测工具\&quot;的相关三元组，三元组内部用\&quot;_\&quot;连接，三元组之间用\\n分割。文本：
输入：332号汽车故障报告故障现象空调系统故障，按空调控制器任何按键都没有反映。
输出：空调系统_部件故障_故障\n空调控制器_部件故障_没有反映
</pre></div>
</div>
<p>单卡训练脚本：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">activate</span> <span class="n">chatglm</span>

<span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="mi">0</span> <span class="n">deepspeed</span> <span class="n">finetuning_freeze</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">5</span> <span class="o">--</span><span class="n">train_batch_size</span> <span class="mi">2</span>
</pre></div>
</div>
<p>多卡训练脚本：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span> <span class="n">deepspeed</span> <span class="n">finetuning_freeze</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">5</span> <span class="o">--</span><span class="n">train_batch_size</span> <span class="mi">2</span>
</pre></div>
</div>
<p>在多卡模式下，训练显存翻倍增加，训练时间变慢很多？？？</p>
<p><strong>P-tuning V2方法</strong></p>
<p>​	是一种针对大模型的soft-prompt方法，P-tuning V2将大模型的Embedding和每一层前加入新的Prompt参数，核心代码：</p>
<p><img alt="" src="../../_images/image-20230509173739968.png" /></p>
<p>当<code class="docutils literal notranslate"><span class="pre">prefix_encoder</span></code>为True时，采用<code class="docutils literal notranslate"><span class="pre">P-Tuning</span> <span class="pre">V2</span></code>方法；为<code class="docutils literal notranslate"><span class="pre">False</span></code>时，采用<code class="docutils literal notranslate"><span class="pre">P-Tuning</span></code>方法，仅在大模型的Embedding上添加参数</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">CUDA_VISIBLE_DEVICES</span><span class="o">=</span><span class="mi">0</span> <span class="n">deepspeed</span> <span class="n">finetuning_pt</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">5</span> <span class="o">--</span><span class="n">train_batch_size</span> <span class="mi">2</span> <span class="o">--</span><span class="n">pre_seq_len</span> <span class="mi">16</span>
</pre></div>
</div>
<p><strong>LoRA方法</strong></p>
<p>Lora方法，即在大型语言模型上对指定参数（权重矩阵）并行增加额外的低秩矩阵，并在模型训练过程中，仅训练额外增加的并行低秩矩阵的参数。 当“秩值”远小于原始参数维度时，新增的低秩矩阵参数量也就很小。在下游任务tuning时，仅须训练很小的参数，但能获取较好的表现结果。核心代码：</p>
<p><img alt="" src="../../_images/image-20230511203445674.png" /></p>
<p><strong>测试结果</strong></p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>微调方法</p></th>
<th class="head"><p>未微调</p></th>
<th class="head"><p>Freeze(fp16)</p></th>
<th class="head"><p>PT(int 4)</p></th>
<th class="head"><p>LoRA(load_in_int8)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>训练显存占用</p></td>
<td><p></p></td>
<td><p>22.938GiB</p></td>
<td><p>21.894GiB</p></td>
<td><p>22.662GiB</p></td>
</tr>
<tr class="row-odd"><td><p>可训练参数</p></td>
<td><p></p></td>
<td><p>1.01B</p></td>
<td><p>0.96B</p></td>
<td><p>0.0367B</p></td>
</tr>
<tr class="row-even"><td><p>总参数</p></td>
<td><p></p></td>
<td><p>6.17B</p></td>
<td><p>4.31B</p></td>
<td><p>6.17B</p></td>
</tr>
<tr class="row-odd"><td><p>可训练参数比</p></td>
<td><p></p></td>
<td><p>16.31%</p></td>
<td><p>22.18%</p></td>
<td><p>0.06%</p></td>
</tr>
<tr class="row-even"><td><p>推理显存占用</p></td>
<td><p></p></td>
<td><p>13.174GiB</p></td>
<td><p>7.788GiB</p></td>
<td><p>9.184GiB</p></td>
</tr>
<tr class="row-odd"><td><p>训练耗时</p></td>
<td><p></p></td>
<td><p>78.86min</p></td>
<td><p>107.6min</p></td>
<td><p>73.5min</p></td>
</tr>
<tr class="row-even"><td><p>测试结果F1</p></td>
<td><p>0.0</p></td>
<td><p>0.5467</p></td>
<td><p>0.6211</p></td>
<td><p>0.5393</p></td>
</tr>
<tr class="row-odd"><td><p>测试耗时</p></td>
<td><p>113.215s</p></td>
<td><p>44.161s</p></td>
<td><p>96.128s</p></td>
<td><p>172.885s</p></td>
</tr>
</tbody>
</table>
<p><strong>遇到的问题</strong></p>
<ul class="simple">
<li><p>bitsandbytes</p></li>
</ul>
<p>在模型加载中使用<code class="docutils literal notranslate"><span class="pre">load_in_8bit=True</span></code>时，要求安装accelerate和bitsandbytes</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">ChatGLMForConditionalGeneration</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">model_dir</span><span class="p">,</span> <span class="n">load_in_8bit</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">trust_remote_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">device_map</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>对于CUDA11.8，accelerate使用pip进行安装，直接使用<code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">bitsandbytes</span></code>安装只能安装到cu117版本，需要采用源码安装：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">timdettmers</span><span class="o">/</span><span class="n">bitsandbytes</span><span class="o">.</span><span class="n">git</span>
<span class="n">cd</span> <span class="n">bitsandbytes</span>

<span class="c1"># CUDA_VERSIONS in {110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 120}</span>
<span class="c1"># make argument in {cuda110, cuda11x, cuda12x}</span>
<span class="c1"># if you do not know what CUDA you have, try looking at the output of: python -m bitsandbytes</span>
<span class="n">CUDA_VERSION</span><span class="o">=</span><span class="mi">117</span> <span class="n">make</span> <span class="n">cuda11x</span>
<span class="n">python</span> <span class="n">setup</span><span class="o">.</span><span class="n">py</span> <span class="n">install</span>
</pre></div>
</div>
<ul>
<li><p>训练LoRA时，随着训练轮次的增加，显存占用也会增加，容易爆显存，以下两种方法可以有所缓解</p>
<ol class="arabic">
<li><p>设置<code class="docutils literal notranslate"><span class="pre">max_split_size_mb:128</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PYTORCH_CUDA_ALLOC_CONF&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;max_split_size_mb:128&quot;</span>
</pre></div>
</div>
</li>
<li><p>deepspeed在<code class="docutils literal notranslate"><span class="pre">zero_optimization</span></code>下添加设置</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;offload_param&quot;</span><span class="p">:</span> <span class="p">{</span>
	<span class="s2">&quot;device&quot;</span><span class="p">:</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
	<span class="s2">&quot;pin_memory&quot;</span><span class="p">:</span> <span class="kc">True</span>
<span class="p">},</span>
</pre></div>
</div>
</li>
</ol>
</li>
</ul>
</section>

</main>
                        <nav aria-label="Page navigation" class="py-4 my-5 clearfix font-weight-bold border-top">
    <a class="float-left" href="03Lora.html" title="Previous">
        <span aria-hidden="true">←&nbsp;</span>LoRA
    </a>
    <a class="float-right" href="../2-index.html" title="Next">
        语音识别 <span aria-hidden="true">&nbsp;→</span>
    </a>
</nav>
                    </div>
                    
                </div>
            </div>
        </div>
    </div>
    <footer class="container-fluid bg-primary text-light">
        <div class="container">
            <div class="row">
        <div class="col p-4">
            
                <nav aria-label="Footer">
                    <ul class="nav justify-content-center mb-2">
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/features/">Features</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/about-wagtail/"> About Wagtail</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/services/"> Services</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/blog/"> Blog</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/packages/"> Packages</a></li>
                        
                            
                            
                            <li class="nav-item"><a class="nav-link text-light"  href="https://wagtail.org/developers/"> Developers</a></li>
                        
                    </ul>
                </nav>
            
            <div class="text-center">
                <p style="display: none">
                    <a class="text-light" href="https://github.com/wagtail/sphinx_wagtail_theme" rel="nofollow" target="_blank">
                        Wagtail Sphinx Theme 6.0.0
                    </a>
                </p>
            </div>
            <div class="text-center">
                    &copy; Copyright 2023, 李仲亮
            </div>
        </div>
    </div>
        </div>
    </footer>
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/sphinx_highlight.js"></script>
        <script src="../../_static/clipboard.min.js"></script>
        <script src="../../_static/copybutton.js"></script>
        <script src="../../_static/translations.js"></script>
        <script type="text/javascript" src="../../_static/dist/theme.js"></script>
        <script type="text/javascript" src="../../_static/dist/vendor.js"></script>
        <script type="text/javascript" src="../../_static/searchtools.js"></script>
        <script type="text/javascript" src="../../_static/language_data.js"></script>
        <script type="text/javascript">
            document.addEventListener('DOMContentLoaded', function() { Search.loadIndex("../../searchindex.js"); });
        </script>
        <script type="text/javascript" id="searchindexloader"></script>
    </body>
</html>